# -*- coding: utf-8 -*-
import concurrent.futures
import hashlib
import sys
from datetime import datetime, timedelta
from glob import glob
import pandas as pd
import requests
from tqdm import tqdm
from config import *
import random
from pathlib import Path


def extract_coin_names(folder_path):
    zip_files = glob(os.path.join(folder_path, '*.zip'))
    coin_names = set()  # ä½¿ç”¨é›†åˆæ¥é¿å…é‡å¤çš„å¸ç§åç§°
    for zip_file in zip_files:
        coin_name = os.path.basename(zip_file).split('-')[0]
        coin_names.add(coin_name)
    coin_names_list = list(coin_names)
    coin_names_list.sort()
    return coin_names_list


def get_all_symbols(proxies, target):
    max_retries = 5
    retries = 0
    # æ ¹æ® target çš„å€¼é€‰æ‹©åˆé€‚çš„ URL
    url = 'https://data-api.binance.vision/api/v3/exchangeInfo' if target == 'spot' else 'https://fapi.binance.com/fapi/v1/exchangeInfo'

    while retries < max_retries:
        try:
            # å‘é€è¯·æ±‚åˆ° Binance API
            response = requests.get(url, proxies=proxies)
            response.raise_for_status()  # æ£€æŸ¥è¯·æ±‚æ˜¯å¦æˆåŠŸ
            data = response.json()  # è§£æ JSON æ•°æ®
            symbols = [market['symbol'] for market in data['symbols'] if market['symbol'].endswith('USDT')]
            return symbols

        except requests.exceptions.Timeout as e:

            print(f"è¯·æ±‚è¶…æ—¶ï¼Œæ­£åœ¨é‡è¯•...({retries + 1}/{max_retries})", e)
            retries += 1  # é‡è¯•æ¬¡æ•°å¢åŠ 

        except requests.exceptions.RequestException as e:

            print(f"ç½‘ç»œæˆ–è¯·æ±‚é”™è¯¯ï¼Œæ­£åœ¨é‡è¯•...({retries + 1}/{max_retries})", e)
            retries += 1
    return []  # æ‰€æœ‰é‡è¯•å°è¯•åä»ç„¶å¤±è´¥ï¼Œè¿”å›ç©ºåˆ—è¡¨


def get_active_symbols(proxies, target):
    try:
        # å‘é€è¯·æ±‚åˆ° Binance API  https://api.binance.com/api/v3/exchangeInfo
        res_spot = requests.get('https://data-api.binance.vision/api/v3/exchangeInfo', proxies=proxies)
        res_swap = requests.get('https://fapi.binance.com/fapi/v1/exchangeInfo', proxies=proxies)
        res_spot.raise_for_status()  # æ£€æŸ¥è¯·æ±‚æ˜¯å¦æˆåŠŸ
        res_swap.raise_for_status()  # æ£€æŸ¥è¯·æ±‚æ˜¯å¦æˆåŠŸ
        data_spot = res_spot.json()  # è§£æ JSON æ•°æ®
        data_swap = res_swap.json()  # è§£æ JSON æ•°æ®
        symbols = []

        if target == 'spot':
            # å¤„ç†ç°è´§å¸‚åœºçš„äº¤æ˜“å¯¹
            for market in data_spot['symbols']:
                if market['symbol'].endswith('USDT') and market['status'] == 'TRADING':
                    symbols.append(market['symbol'])

        elif target == 'swap':
            # å¤„ç†æ°¸ç»­åˆçº¦å¸‚åœºçš„äº¤æ˜“å¯¹
            for market in data_swap['symbols']:
                if market['symbol'].endswith('USDT') and market['status'] == 'TRADING':
                    symbols.append(market['symbol'])

    except requests.exceptions.Timeout as e:
        print("è¯·æ±‚è¶…æ—¶:", e)
        return []
    except requests.exceptions.RequestException as e:
        print("ç½‘ç»œæˆ–è¯·æ±‚é”™è¯¯:", e)
        return []

    return symbols


def is_full_month(date_range, year, month):
    month_start = datetime(year, month, 1)
    if month == 12:
        month_end = datetime(year + 1, 1, 1) - timedelta(days=1)
    else:
        month_end = datetime(year, month + 1, 1) - timedelta(days=1)

    current_date = month_start
    while current_date <= month_end:
        if current_date not in date_range:
            return False
        current_date += timedelta(days=1)
    return True


def is_url_accessible(btcurl):
    try:
        response = requests.head(btcurl, allow_redirects=True)
        return response.status_code == 200
    except requests.RequestException:
        return False


# ä¸‹è½½URLçš„å‡½æ•°
def download_url(url, directory, proxies):
    max_retries = 10  # è®¾ç½®æœ€å¤§é‡è¯•æ¬¡æ•°
    retries = 0

    while retries < max_retries:
        filename = url.split('/')[-1]
        file_path = os.path.join(directory, filename)

        try:
            response = requests.get(url, proxies=proxies, stream=True)
            response.raise_for_status()
            with open(file_path, 'wb') as f:
                for chunk in response.iter_content(chunk_size=8192):
                    f.write(chunk)
            return retries
        except requests.RequestException as e:
            if "Not Found for url" in str(e):  # æ£€æŸ¥é”™è¯¯ä¿¡æ¯æ˜¯å¦åŒ…å« "Not Found for url"
                # print(f"{filename.split('.zip')[0]},æ­¤æ—¶æœŸæ— Kçº¿æ•°æ®")
                return -1

            else:
                retries += 1
    return retries


# ä¸»ä¸‹è½½é€»è¾‘
def main_download(urls, directory, proxies):
    error_urls = []
    success_urls = []
    retryed_urls = []
    with concurrent.futures.ThreadPoolExecutor(max_workers=ä¸‹è½½çº¿ç¨‹æ•°) as executor:
        future_to_url = {executor.submit(download_url, url, directory, proxies): url for url in urls}
        for future in concurrent.futures.as_completed(future_to_url):
            url = future_to_url[future]
            filename = url.split('/')[-1]
            result = future.result()
            if result == 10:
                error_urls.append(f'ï¼ï¼ï¼{filename}æœ€ç»ˆä¸‹è½½å¤±è´¥ï¼ï¼ï¼,é‡è¯•{result}æ¬¡,{url}')
            elif result > 0:
                retryed_urls.append(f'{filename}ä¸‹è½½æˆåŠŸ,é‡è¯•æ¬¡æ•°ï¼š{result}')
                success_urls.append(url)
            elif result == 0:
                success_urls.append(url)
    return error_urls, retryed_urls, success_urls


def verify_checksum(zip_file_path, checksum_file_path):
    sha256_hash = hashlib.sha256()
    with open(zip_file_path, 'rb') as f:
        for byte_block in iter(lambda: f.read(4096), b""):
            sha256_hash.update(byte_block)
    zip_file_hash = sha256_hash.hexdigest()

    with open(checksum_file_path, 'r') as file:
        checksum_file_hash = file.read().split()[0]

    return zip_file_hash == checksum_file_hash


def all_merge_csv(folder_path):
    matching_files = list(Path(folder_path).glob(f"*{'_merge'}*.csv"))
    merge_files = set()  # ä½¿ç”¨é›†åˆæ¥é¿å…é‡å¤çš„å¸ç§åç§°

    # ç»Ÿè®¡å·²å­˜åœ¨å¤šå°‘ä¸ª _merge çš„æ–‡ä»¶
    existing_merge_files_count = len(matching_files)
    if existing_merge_files_count > 0:
        print(f"æ£€æŸ¥åˆ°ä¸Šæ¬¡æœ‰ä»»åŠ¡ä¸­æ–­ã€‚ä¸Šæ¬¡å·²å®Œæˆ {existing_merge_files_count} ä¸ªå¸ç§çš„æ¸…æ´—åˆå¹¶ä»»åŠ¡ï¼Œå¼€å§‹ç»§ç»­ä¸‹è½½.")

    for merge_file in matching_files:
        coin_name = os.path.basename(merge_file).split('_')[0]
        merge_files.add(coin_name)

    merge_files_list = list(merge_files)
    merge_files_list.sort()

    # åˆ é™¤é™¤äº†å¸¦æœ‰ _merge åç¼€çš„æ–‡ä»¶ä¹‹å¤–çš„å…¶ä»– CSV æ–‡ä»¶
    for file in Path(folder_path).glob("*.csv"):
        if "_merge" not in file.name:
            file.unlink()

    return merge_files_list


if __name__ == '__main__':
    # é»˜è®¤å€¼
    target = 'spot'
    # æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿçš„å‘½ä»¤è¡Œå‚æ•°
    if len(sys.argv) > 1:
        target = sys.argv[1]

    # ä½ å¯ä»¥åœ¨è¿™é‡Œæ ¹æ®targetçš„å€¼è¿›è¡Œç›¸åº”çš„æ“ä½œ
    if target == "spot":
        download_directory = ç°è´§ä¸´æ—¶ä¸‹è½½æ–‡ä»¶å¤¹
        data_directory = ç°è´§Kçº¿å­˜æ”¾è·¯å¾„
        base_url = 'https://data.binance.vision/data/' + target
        mode = "ç°è´§"
    if target == "swap":
        download_directory = æ°¸ç»­åˆçº¦ä¸´æ—¶ä¸‹è½½æ–‡ä»¶å¤¹
        data_directory = æ°¸ç»­åˆçº¦Kçº¿å­˜æ”¾è·¯å¾„
        base_url = 'https://data.binance.vision/data/futures/um'
        mode = "åˆçº¦"
    checksum_directory = os.path.join(download_directory, 'checksums')
    os.makedirs(checksum_directory, exist_ok=True)
    # è®¾ç½®å¢é‡zipæ–‡ä»¶ä¸‹è½½ç›®å½•
    failed_symbols_log = os.path.join(main_path,  f'{mode}_Download_failed_log.txt')
    retryed_symbols_log = os.path.join(main_path,  f'{mode}_Download_retryed_log.txt')
    Verify_times_log = os.path.join(main_path,  f'{mode}_Verify_checksum_times_log.txt')
    # ä¸ºæ¯ä¸ªå¸ç§ç”ŸæˆURL
    print(f"å³å°†ä¸‹è½½ {mode}Kçº¿æ•°æ®")
    print(f'ä½¿ç”¨çš„ä¸‹è½½æ¥å£ä¸º:{base_url}')
    symbols = get_all_symbols(proxies, target)  # ä¸‹è½½å…¨éƒ¨å¸ç§,åŒ…æ‹¬ç°åœ¨å·²ç»ä¸‹æ¶çš„
    # è¯»å–CSVæ–‡ä»¶ï¼Œè·å–æœ€æ–°çš„candle_begin_timeæ—¥æœŸ
    csv_path = os.path.join(data_directory, 'BTC-USDT.csv')
    if os.path.exists(csv_path):
        df = pd.read_csv(csv_path, skiprows=1, encoding='GBK')
        latest_date_str = df['candle_begin_time'].max()
        latest_date = datetime.strptime(latest_date_str, '%Y-%m-%d %H:%M:%S')
        print(f'å¸å®‰åœ¨äº¤æ˜“çš„{target}USDTäº¤æ˜“å¯¹å¸ç§æ€»æ•°:', len(symbols))
    else:
        latest_date = datetime(2017, 9, 3)
        # ç”Ÿæˆå¸ç§åˆ—è¡¨

    if len(symbols) < 1:
        exit()

    print(f'å¸å®‰å…¨éƒ¨{mode}USDTäº¤æ˜“å¯¹å¸ç§ä¸ªæ•°:', len(symbols))
    symbols = [symbol for symbol in symbols if not any(keyword in symbol for keyword in ['UP', 'DOWN', 'BEAR', 'BULL'])]
    print(f'å»é™¤æ æ†ä»£å¸åçš„{mode}å¸ç§ä¸ªæ•°:', len(symbols))
    symbols.sort()
    if debug_mode:
        symbols = symbols[:5]  # è°ƒè¯•è¯­å¥

    # ===æŒ‡å®šä¸‹è½½åˆ—è¡¨çš„ä¸­æ–­ç‚¹ï¼Œç”¨äºæ„å¤–ä¸­æ–­åçš„ç»­ä¼ 
    coins_already_download = extract_coin_names(download_directory)

    if len(coins_already_download) > 0:
        index_next = symbols.index(coins_already_download[-1])
        symbols = symbols[index_next:]

    print(f'å³å°†ä¸‹è½½çš„{mode}å¸ç§æ€»ä¸ªæ•°:', len(symbols))

    merges = all_merge_csv(download_directory)
    if merges:
        symbols = sorted(list(set(symbols) - set(merges)))
        print('éœ€è¦è¡¥å……ä¸‹è½½çš„å¸ç§:', symbols)
        print('éœ€è¦è¡¥å……ä¸‹è½½çš„å¸ç§ä¸ªæ•°:', len(symbols))

    # è·å–å½“å‰æ—¥æœŸ
    current_date = datetime.now()

    # è®¡ç®—æ—¥æœŸèŒƒå›´
    start_date = latest_date - timedelta(days=2)


    date_range = [start_date + timedelta(days=i) for i in range((current_date - start_date).days + 1 )]
    print(f'ä¸‹è½½{mode}Kçº¿æ•°æ®çš„æ—¥æœŸèµ·ç‚¹:', start_date)
    print(f'ä¸‹è½½{mode}Kçº¿æ•°æ®çš„æ—¥æœŸç»ˆç‚¹:', date_range[-1])

    # è®¡ç®—ä¸Šä¸€ä¸ªæœˆçš„å¹´ä»½å’Œæœˆä»½
    current_year, current_month = datetime.now().year, datetime.now().month
    previous_year, previous_month = (current_year - 1, 12) if current_month == 1 else (current_year, current_month - 1)

    # æ„å»ºä¸Šä¸€ä¸ªæœˆçš„æœˆåº¦æ•°æ®URL
    previous_month_url = f"{base_url}/monthly/klines/BTCUSDT/{interval}/BTCUSDT-{interval}-{previous_year}-{str(previous_month).zfill(2)}.zip"

    # æ£€æŸ¥ä¸Šä¸€ä¸ªæœˆçš„URLæ˜¯å¦å¯è®¿é—®
    previous_month_accessible = is_url_accessible(previous_month_url)

    # åˆ†ç»„ä¸ºæœˆåº¦ä¸‹è½½å’Œæ—¥åº¦ä¸‹è½½
    monthly_download = set()

    daily_download = []
    for date in date_range:
        year_month = (date.year, date.month)
        if is_full_month(date_range, date.year, date.month) and (
                year_month != (previous_year, previous_month) or previous_month_accessible):
            monthly_download.add(year_month)
        else:
            daily_download.append(date)
    monthly_download = sorted(monthly_download, key=lambda x: (x[0], x[1]))
    daily_download.sort()

    pbar = tqdm(symbols, desc=f"ğŸ“ˆ å¼€å§‹ä¸‹è½½{symbols[0]}...", unit=f"{mode}")
    for symbol in pbar:
        urls = []
        checksum_urls = []
        # æ·»åŠ æœˆåº¦æ•°æ®URL
        for year, month in monthly_download:
            url = f"{base_url}/monthly/klines/{symbol}/{interval}/{symbol}-{interval}-{year}-{str(month).zfill(2)}.zip"
            urls.append(url)
            checksum_urls.append(url + '.CHECKSUM')
        # # æ·»åŠ æ—¥åº¦æ•°æ®URL
        for date in daily_download:
            date_str = date.strftime('%Y-%m-%d')
            url = f"{base_url}/daily/klines/{symbol}/{interval}/{symbol}-{interval}-{date_str}.zip"
            urls.append(url)
            checksum_urls.append(url + '.CHECKSUM')

        # å»é‡URLs
        urls = list(set(urls))
        urls.sort()

        checksum_urls = list(set(checksum_urls))
        checksum_urls.sort()

        # ä¸‹è½½å¤±è´¥çš„urlåˆ—è¡¨
        failed_symbols, retryed_symbols, success_symbol_urls = main_download(urls, download_directory, proxies)
        # ä¸‹è½½å¤±è´¥çš„CHECKSUMæ–‡ä»¶çš„urlåˆ—è¡¨
        failed_checksums, retryed_checksums, success_checksums = main_download(checksum_urls, checksum_directory, proxies)

        # åˆå§‹åŒ–ç”¨äºè·Ÿè¸ªæ¯ä¸ªå¸ç§å¤±è´¥æ¬¡æ•°çš„å­—å…¸
        if len(failed_symbols) > 0:
            with open(failed_symbols_log, 'a') as f:
                for i in failed_symbols:
                    f.write(f'{i}\n')
        if len(retryed_symbols) > 0:
            with open(retryed_symbols_log, 'a') as f:
                for i in retryed_symbols:
                    f.write(f'{i}\n')

        for url in success_symbol_urls:
            filename = url.split('/')[-1]
            zip_file_path = os.path.join(download_directory, filename)
            checksum_file_path = os.path.join(checksum_directory, filename + '.CHECKSUM')

            valid = False
            attempts = 0
            max_attempts = 10
            while not valid and attempts < max_attempts:
                if os.path.exists(zip_file_path) and os.path.exists(checksum_file_path):
                    # ä½¿ç”¨verify_checksumå‡½æ•°éªŒè¯æ–‡ä»¶
                    valid = verify_checksum(zip_file_path, checksum_file_path)

                    if not valid:
                        # å¦‚æœæ ¡éªŒå¤±è´¥ï¼Œè®°å½•è¯¥å¸ç§çš„å¤±è´¥æ¬¡æ•°
                        symbol = filename.split('_')[0]

                        # åˆ é™¤åŸæœ‰æ–‡ä»¶ï¼Œä»¥ä¾¿é‡æ–°ä¸‹è½½
                        os.remove(zip_file_path)
                        os.remove(checksum_file_path)

                        # é‡æ–°ä¸‹è½½ZIPæ–‡ä»¶å’ŒCHECKSUMæ–‡ä»¶
                        download_url(url, download_directory, proxies)
                        download_url(url + '.CHECKSUM', checksum_directory, proxies)

                attempts += 1
            # æ ¡éªŒç»“æŸï¼Œè®°å½•é‡è¯•æ¬¡æ•°å¤§äº1çš„æƒ…å†µ
            if attempts > 1:  # ç­‰äº1æ—¶ï¼Œä»£è¡¨ç»è¿‡ä¸€æ¬¡æ ¡éªŒå³é€šè¿‡
                with open(Verify_times_log, 'a') as f:  # ä½¿ç”¨è¿½åŠ æ¨¡å¼'a'
                    f.write(f"{symbol}: {filename}, æ ¡éªŒé‡è¯•æ¬¡æ•°: {attempts - 1}\n")

        matching_files = list(Path(download_directory).glob(f"*{symbol}*.zip"))
        num_matching_files = len(matching_files)
        emoji_options = ["âœ…", "ğŸ‰", "ğŸŒŸ", "ğŸš€", "ğŸ’¡", "ğŸ”¥", "ğŸŒˆ", "ğŸ’", "ğŸ˜", "ğŸŒ¸", ]
        random_emoji = random.choice(emoji_options)
        coin_name = symbol.replace("USDT", "-USDT")
        pbar.set_description(f"{random_emoji}{coin_name} æˆåŠŸä¸‹è½½å¹¶é€šè¿‡æ ¡éªŒ,åŒ…å«{num_matching_files}ä¸ª.zipæ–‡ä»¶")

    pbar.close()

