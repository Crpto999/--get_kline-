import concurrent.futures
import hashlib
import os
import sys
from datetime import datetime, timedelta
from glob import glob
import pandas as pd
import requests
from tqdm import tqdm
from config import *
import random
from pathlib import Path

def extract_coin_names(folder_path):
    zip_files = glob(os.path.join(folder_path, '*.zip'))
    coin_names = set()  # ä½¿ç”¨é›†åˆæ¥é¿å…é‡å¤çš„å¸ç§åç§°
    for zip_file in zip_files:
        coin_name = os.path.basename(zip_file).split('-')[0]
        coin_names.add(coin_name)
    coin_names_list = list(coin_names)
    coin_names_list.sort()
    return coin_names_list


def get_all_symbols(proxies, target):
    max_retries = 5
    retries = 0
    # æ ¹æ® target çš„å€¼é€‰æ‹©åˆé€‚çš„ URL
    url = 'https://data-api.binance.vision/api/v3/exchangeInfo' if target == 'spot' else 'https://fapi.binance.com/fapi/v1/exchangeInfo'

    while retries < max_retries:
        try:
            # å‘é€è¯·æ±‚åˆ° Binance API
            response = requests.get(url, proxies=proxies)
            response.raise_for_status()  # æ£€æŸ¥è¯·æ±‚æ˜¯å¦æˆåŠŸ
            data = response.json()  # è§£æ JSON æ•°æ®
            symbols = [market['symbol'] for market in data['symbols'] if market['symbol'].endswith('USDT')]
            return symbols

        except requests.exceptions.Timeout as e:

            print(f"è¯·æ±‚è¶…æ—¶ï¼Œæ­£åœ¨é‡è¯•...({retries + 1}/{max_retries})", e)
            retries += 1  # é‡è¯•æ¬¡æ•°å¢åŠ 

        except requests.exceptions.RequestException as e:

            print(f"ç½‘ç»œæˆ–è¯·æ±‚é”™è¯¯ï¼Œæ­£åœ¨é‡è¯•...({retries + 1}/{max_retries})", e)
            retries += 1
    return []  # æ‰€æœ‰é‡è¯•å°è¯•åä»ç„¶å¤±è´¥ï¼Œè¿”å›ç©ºåˆ—è¡¨


def get_active_symbols(proxies, target):
    try:
        # å‘é€è¯·æ±‚åˆ° Binance API  https://api.binance.com/api/v3/exchangeInfo
        res_spot = requests.get('https://data-api.binance.vision/api/v3/exchangeInfo', proxies=proxies)
        res_swap = requests.get('https://fapi.binance.com/fapi/v1/exchangeInfo', proxies=proxies)
        res_spot.raise_for_status()  # æ£€æŸ¥è¯·æ±‚æ˜¯å¦æˆåŠŸ
        res_swap.raise_for_status()  # æ£€æŸ¥è¯·æ±‚æ˜¯å¦æˆåŠŸ
        data_spot = res_spot.json()  # è§£æ JSON æ•°æ®
        data_swap = res_swap.json()  # è§£æ JSON æ•°æ®
        symbols = []

        if target == 'spot':
            # å¤„ç†ç°è´§å¸‚åœºçš„äº¤æ˜“å¯¹
            for market in data_spot['symbols']:
                if market['symbol'].endswith('USDT') and market['status'] == 'TRADING':
                    symbols.append(market['symbol'])

        elif target == 'swap':
            # å¤„ç†æ°¸ç»­åˆçº¦å¸‚åœºçš„äº¤æ˜“å¯¹
            for market in data_swap['symbols']:
                if market['symbol'].endswith('USDT') and market['status'] == 'TRADING':
                    symbols.append(market['symbol'])

    except requests.exceptions.Timeout as e:
        print("è¯·æ±‚è¶…æ—¶:", e)
        return []
    except requests.exceptions.RequestException as e:
        print("ç½‘ç»œæˆ–è¯·æ±‚é”™è¯¯:", e)
        return []

    return symbols


def is_full_month(date_range, year, month):
    month_start = datetime(year, month, 1)
    if month == 12:
        month_end = datetime(year + 1, 1, 1) - timedelta(days=1)
    else:
        month_end = datetime(year, month + 1, 1) - timedelta(days=1)

    current_date = month_start
    while current_date <= month_end:
        if current_date not in date_range:
            return False
        current_date += timedelta(days=1)
    return True


def is_url_accessible(btcurl):
    try:
        response = requests.head(btcurl, allow_redirects=True)
        return response.status_code == 200
    except requests.RequestException:
        return False


# ä¸‹è½½URLçš„å‡½æ•°
def download_url(url, directory, proxies):
    max_retries = 10  # è®¾ç½®æœ€å¤§é‡è¯•æ¬¡æ•°
    retries = 0

    while retries < max_retries:
        filename = url.split('/')[-1]
        file_path = os.path.join(directory, filename)
        try:
            response = requests.get(url, proxies=proxies, stream=True)
            response.raise_for_status()
            with open(file_path, 'wb') as f:
                for chunk in response.iter_content(chunk_size=8192):
                    f.write(chunk)
            return True
        except requests.RequestException as e:
            if "Not Found for url" in str(e):  # æ£€æŸ¥é”™è¯¯ä¿¡æ¯æ˜¯å¦åŒ…å« "Not Found for url"
                # print(f"{filename.split('.zip')[0]},æ­¤æ—¶æœŸæ— Kçº¿æ•°æ®")
                return 'NotFound'

            else:
                retries += 1
                # print(f"ä¸‹è½½ {url} å¤±è´¥ï¼Œé‡è¯•ç¬¬{retries}æ¬¡...")
    # å¦‚æœæ‰€æœ‰å°è¯•éƒ½å¤±è´¥ï¼Œè¿”å›False
    return False


# ä¸»ä¸‹è½½é€»è¾‘
def main_download(urls, directory, proxies):
    error_urls = []
    success_urls = []
    with concurrent.futures.ThreadPoolExecutor(max_workers=16) as executor:
        future_to_url = {executor.submit(download_url, url, directory, proxies): url for url in urls}
        for future in concurrent.futures.as_completed(future_to_url):
            url = future_to_url[future]
            result = future.result()
            if result is False:
                error_urls.append(url)
            if result is True:
                success_urls.append(url)
    return error_urls, success_urls


def verify_checksum(zip_file_path, checksum_file_path):
    sha256_hash = hashlib.sha256()
    with open(zip_file_path, 'rb') as f:
        for byte_block in iter(lambda: f.read(4096), b""):
            sha256_hash.update(byte_block)
    zip_file_hash = sha256_hash.hexdigest()

    with open(checksum_file_path, 'r') as file:
        checksum_file_hash = file.read().split()[0]

    return zip_file_hash == checksum_file_hash


def all_merge_csv(folder_path):
    matching_files = list(Path(folder_path).glob(f"*{'_merge'}*.csv"))
    merge_files = set()  # ä½¿ç”¨é›†åˆæ¥é¿å…é‡å¤çš„å¸ç§åç§°

    # ç»Ÿè®¡å·²å­˜åœ¨å¤šå°‘ä¸ª _merge çš„æ–‡ä»¶
    existing_merge_files_count = len(matching_files)
    if existing_merge_files_count > 0:
        print(f"æ£€æŸ¥åˆ°ä¸Šæ¬¡æœ‰ä»»åŠ¡ä¸­æ–­ã€‚ä¸Šæ¬¡å·²å®Œæˆ {existing_merge_files_count} ä¸ªå¸ç§çš„æ¸…æ´—åˆå¹¶ä»»åŠ¡ï¼Œå¼€å§‹ç»§ç»­ä¸‹è½½.")

    for merge_file in matching_files:
        coin_name = os.path.basename(merge_file).split('_')[0]
        merge_files.add(coin_name)

    merge_files_list = list(merge_files)
    merge_files_list.sort()

    # åˆ é™¤é™¤äº†å¸¦æœ‰ _merge åç¼€çš„æ–‡ä»¶ä¹‹å¤–çš„å…¶ä»– CSV æ–‡ä»¶
    for file in Path(folder_path).glob("*.csv"):
        if "_merge" not in file.name:
            file.unlink()

    return merge_files_list


if __name__ == '__main__':
    # é»˜è®¤å€¼
    target = 'spot'
    # æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿçš„å‘½ä»¤è¡Œå‚æ•°
    if len(sys.argv) > 1:
        target = sys.argv[1]
        print(f"æ­£åœ¨ä¸‹è½½ {target}çš„Kçº¿æ•°æ®")
    # ä½ å¯ä»¥åœ¨è¿™é‡Œæ ¹æ®targetçš„å€¼è¿›è¡Œç›¸åº”çš„æ“ä½œ
    if target == "spot":
        data_directory = ç°è´§Kçº¿å­˜æ”¾è·¯å¾„
        base_url = 'https://data.binance.vision/data/' + target
    if target == "swap":
        data_directory = æ°¸ç»­åˆçº¦Kçº¿å­˜æ”¾è·¯å¾„
        base_url = 'https://data.binance.vision/data/futures/um'
    # è®¾ç½®å¢é‡zipæ–‡ä»¶ä¸‹è½½ç›®å½•
    download_directory = ä¸‹è½½æ–‡ä»¶å¤¹
    failed_symbols_log = os.path.join(main_path, f'Download_failed_{target}_symbols.txt')
    failed_zips_log = os.path.join(main_path, f'Verify_checksum_failed_{target}_symbols.txt')
    Verify_times_log = os.path.join(main_path, f'Verify_checksum_times_{target}_symbols.txt')
    # ä¸ºæ¯ä¸ªå¸ç§ç”ŸæˆURL

    print(f'ä¸‹è½½APIæ¥å£ä¸º:{base_url}')
    symbols = get_all_symbols(proxies, target)  # ä¸‹è½½å…¨éƒ¨å¸ç§,åŒ…æ‹¬ç°åœ¨å·²ç»ä¸‹æ¶çš„
    # è¯»å–CSVæ–‡ä»¶ï¼Œè·å–æœ€æ–°çš„candle_begin_timeæ—¥æœŸ
    csv_path = os.path.join(data_directory, 'BTC-USDT.csv')
    if os.path.exists(csv_path):
        df = pd.read_csv(csv_path, skiprows=1, encoding='GBK')
        latest_date_str = df['candle_begin_time'].max()
        latest_date = datetime.strptime(latest_date_str, '%Y-%m-%d %H:%M:%S')
        print(f'å¸å®‰åœ¨äº¤æ˜“çš„{target}USDTäº¤æ˜“å¯¹å¸ç§æ€»æ•°:', len(symbols))
    else:
        latest_date = datetime(2017, 9, 3)
        # ç”Ÿæˆå¸ç§åˆ—è¡¨

    if len(symbols) < 1:
        exit()

    print(f'å¸å®‰å…¨éƒ¨{target}USDTäº¤æ˜“å¯¹å¸ç§ä¸ªæ•°:', len(symbols))
    symbols = [symbol for symbol in symbols if not any(keyword in symbol for keyword in ['UP', 'DOWN', 'BEAR', 'BULL'])]
    print('å»é™¤æ æ†ä»£å¸åçš„å¸ç§ä¸ªæ•°:', len(symbols))
    symbols.sort()
    # ===æŒ‡å®šä¸‹è½½åˆ—è¡¨çš„ä¸­æ–­ç‚¹ï¼Œç”¨äºæ„å¤–ä¸­æ–­åçš„ç»­ä¼ 
    coins_already_download = extract_coin_names(ä¸‹è½½æ–‡ä»¶å¤¹)

    if len(coins_already_download) > 0:
        index_acausdt = symbols.index(coins_already_download[-1])
        symbols = symbols[index_acausdt:]

    print('å¸ç§æ€»ä¸ªæ•°:', len(symbols))

    merges = all_merge_csv(ä¸‹è½½æ–‡ä»¶å¤¹)
    if merges:
        symbols = sorted(list(set(symbols) - set(merges)))
        print('éœ€è¦è¡¥å……ä¸‹è½½çš„å¸ç§:', symbols)
        print('éœ€è¦è¡¥å……ä¸‹è½½çš„å¸ç§ä¸ªæ•°:', len(symbols))

    # è·å–å½“å‰æ—¥æœŸ
    current_date = datetime.now()

    # è®¡ç®—æ—¥æœŸèŒƒå›´
    start_date = latest_date - timedelta(days=2)

    print('ä¸‹è½½Kçº¿æ•°æ®çš„æ—¥æœŸèµ·ç‚¹:', start_date)
    date_range = [start_date + timedelta(days=i) for i in range((current_date - start_date).days)]

    print('ä¸‹è½½Kçº¿æ•°æ®çš„æ—¥æœŸç»ˆç‚¹:', date_range[-1])

    # åˆ›å»ºä¸‹è½½ç›®å½•
    os.makedirs(download_directory, exist_ok=True)
    checksum_directory = os.path.join(download_directory, 'checksums')
    os.makedirs(checksum_directory, exist_ok=True)

    # è®¡ç®—ä¸Šä¸€ä¸ªæœˆçš„å¹´ä»½å’Œæœˆä»½
    current_year, current_month = datetime.now().year, datetime.now().month
    previous_year, previous_month = (current_year - 1, 12) if current_month == 1 else (current_year, current_month - 1)

    # æ„å»ºä¸Šä¸€ä¸ªæœˆçš„æœˆåº¦æ•°æ®URL
    previous_month_url = f"{base_url}/monthly/klines/BTCUSDT/{interval}/BTCUSDT-{interval}-{previous_year}-{str(previous_month).zfill(2)}.zip"

    # æ£€æŸ¥ä¸Šä¸€ä¸ªæœˆçš„URLæ˜¯å¦å¯è®¿é—®
    previous_month_accessible = is_url_accessible(previous_month_url)

    # åˆ†ç»„ä¸ºæœˆåº¦ä¸‹è½½å’Œæ—¥åº¦ä¸‹è½½
    monthly_download = set()

    daily_download = []
    for date in date_range:
        year_month = (date.year, date.month)
        if is_full_month(date_range, date.year, date.month) and (
                year_month != (previous_year, previous_month) or previous_month_accessible):
            monthly_download.add(year_month)
        else:
            daily_download.append(date)
    monthly_download = sorted(monthly_download, key=lambda x: (x[0], x[1]))
    daily_download.sort()

    mode = "ç°è´§æ•°æ®" if target == "spot" else "åˆçº¦æ•°æ®"
    pbar = tqdm(symbols, desc="ğŸ“ˆ åˆå§‹åŒ–ä¸‹è½½æ•°æ®", unit=f"{mode}")
    for symbol in pbar:
        urls = []
        checksum_urls = []
        # æ·»åŠ æœˆåº¦æ•°æ®URL
        for year, month in monthly_download:
            url = f"{base_url}/monthly/klines/{symbol}/{interval}/{symbol}-{interval}-{year}-{str(month).zfill(2)}.zip"
            urls.append(url)
            checksum_urls.append(url + '.CHECKSUM')
        # # æ·»åŠ æ—¥åº¦æ•°æ®URL
        for date in daily_download:
            date_str = date.strftime('%Y-%m-%d')
            url = f"{base_url}/daily/klines/{symbol}/{interval}/{symbol}-{interval}-{date_str}.zip"
            urls.append(url)
            checksum_urls.append(url + '.CHECKSUM')

        # å»é‡URLs
        urls = list(set(urls))
        urls.sort()

        checksum_urls = list(set(checksum_urls))
        checksum_urls.sort()

        # ä¸‹è½½å¤±è´¥çš„urlåˆ—è¡¨
        failed_symbol_urls, success_symbol_urls = main_download(urls, download_directory, proxies)
        # ä¸‹è½½å¤±è´¥çš„CHECKSUMæ–‡ä»¶çš„urlåˆ—è¡¨
        failed_checksums, success_checksums = main_download(checksum_urls, checksum_directory, proxies)

        failed_zips = []
        # åˆå§‹åŒ–ç”¨äºè·Ÿè¸ªæ¯ä¸ªå¸ç§å¤±è´¥æ¬¡æ•°çš„å­—å…¸

        for url in success_symbol_urls:
            filename = url.split('/')[-1]
            zip_file_path = os.path.join(download_directory, filename)
            checksum_file_path = os.path.join(checksum_directory, filename + '.CHECKSUM')

            valid = False
            attempts = 0
            max_attempts = 10
            while not valid and attempts < max_attempts:
                if os.path.exists(zip_file_path) and os.path.exists(checksum_file_path):
                    # ä½¿ç”¨verify_checksumå‡½æ•°éªŒè¯æ–‡ä»¶
                    valid = verify_checksum(zip_file_path, checksum_file_path)

                    if not valid:
                        # å¦‚æœæ ¡éªŒå¤±è´¥ï¼Œè®°å½•è¯¥å¸ç§çš„å¤±è´¥æ¬¡æ•°
                        symbol = filename.split('_')[0]  # å‡è®¾æ–‡ä»¶åä»¥å¸ç§å¼€å§‹

                        # åˆ é™¤åŸæœ‰æ–‡ä»¶ï¼Œä»¥ä¾¿é‡æ–°ä¸‹è½½
                        os.remove(zip_file_path)
                        os.remove(checksum_file_path)

                        # é‡æ–°ä¸‹è½½ZIPæ–‡ä»¶å’ŒCHECKSUMæ–‡ä»¶
                        download_url(url, download_directory, proxies)
                        download_url(url + '.CHECKSUM', checksum_directory, proxies)

                attempts += 1
            # æ£€éªŒç»“æŸï¼Œè®°å½•é‡è¯•æ¬¡æ•°å¤§äº1çš„æƒ…å†µ
            if attempts > 1:
                with open(Verify_times_log, 'a') as f:  # ä½¿ç”¨è¿½åŠ æ¨¡å¼'a'
                    f.write(f"{symbol}: {filename}, æ£€éªŒé‡è¯•æ¬¡æ•°: {attempts}\n")

            # å¦‚æœå°è¯•ç»“æŸåè¿˜ä¸é€šè¿‡ï¼Œæ·»åŠ åˆ°å¤±è´¥åˆ—è¡¨
            if not valid:
                failed_zips.append(filename)

        if failed_symbol_urls:
            with open(failed_symbols_log, 'a') as f:
                for url in failed_symbol_urls:
                    f.write(f"{symbol}ä¸‹è½½æœ€ç»ˆå¤±è´¥: {url}\n")

        if failed_zips:
            with open(failed_zips_log, 'a') as f:
                for filename in failed_zips:
                    f.write(f"{symbol}æ£€éªŒæœ€ç»ˆå¤±è´¥: {filename}\n")


        matching_files = list(Path(ä¸‹è½½æ–‡ä»¶å¤¹).glob(f"*{symbol}*.zip"))
        num_matching_files = len(matching_files)
        emoji_options = ["âœ…", "ğŸ‰", "ğŸŒŸ", "ğŸš€", "ğŸ’¡", "ğŸ”¥", "ğŸŒˆ", "ğŸ’", "ğŸ˜", "ğŸŒ¸", ]
        random_emoji = random.choice(emoji_options)
        pbar.set_description(f"{random_emoji}{symbol} æˆåŠŸä¸‹è½½å¹¶é€šè¿‡æ ¡éªŒ,åŒ…å«{num_matching_files}ä¸ª.zipæ–‡ä»¶")

    pbar.close()
    print("\næ‰€æœ‰å¸ç§çš„zipæ–‡ä»¶ä¸‹è½½å®Œæˆ")
