# -*- coding: utf-8 -*-
import os
import shutil
import sys
import time
import zipfile
from glob import glob
from itertools import product
from pathlib import Path
import pandas as pd
from joblib import Parallel, delayed
from tqdm import tqdm
from config import *

pd.set_option('display.max_rows', 1000)
pd.set_option('expand_frame_repr', False)  # å½“åˆ—å¤ªå¤šæ—¶ä¸æ¢è¡Œ


# é€šè¿‡æ‰€æœ‰zipæ–‡ä»¶çš„æ–‡ä»¶åå‰ç¼€è·å–å¸ç§åç§°åˆ—è¡¨
def extract_coin_names(folder_path):
    zip_files = glob(os.path.join(folder_path, '*.zip'))
    print('å‘ç° {} ä¸ªzip æ–‡ä»¶.'.format(len(zip_files)))

    coin_names = set()  # ä½¿ç”¨é›†åˆæ¥é¿å…é‡å¤çš„å¸ç§åç§°
    for zip_file in zip_files:
        coin_name = os.path.basename(zip_file).split('-')[0]
        coin_names.add(coin_name)
    coin_names_list = list(coin_names)
    coin_names_list.sort()
    return coin_names_list


def all_merge_csv(folder_path):
    matching_files = list(Path(folder_path).glob(f"*{'_merge'}*.csv"))
    merge_files = set()  # ä½¿ç”¨é›†åˆæ¥é¿å…é‡å¤çš„å¸ç§åç§°

    # ç»Ÿè®¡å·²å­˜åœ¨å¤šå°‘ä¸ª _merge çš„æ–‡ä»¶
    existing_merge_files_count = len(matching_files)
    if existing_merge_files_count > 2:
        print(f"æ£€æŸ¥åˆ°ä¸Šæ¬¡æœ‰ä»»åŠ¡ä¸­æ–­ã€‚ä¸Šæ¬¡å·²å®Œæˆ {existing_merge_files_count} ä¸ªå¸ç§çš„æ¸…æ´—ä»»åŠ¡ï¼Œå¼€å§‹ç»­æ´—.")

        for merge_file in matching_files:
            coin_name = os.path.basename(merge_file).split('_')[0]
            merge_files.add(coin_name)

        merge_files_list = list(merge_files)
        merge_files_list.sort()

        # åˆ é™¤é™¤äº†å¸¦æœ‰ _merge åç¼€çš„æ–‡ä»¶ä¹‹å¤–çš„å…¶ä»– CSV æ–‡ä»¶
        for file in Path(folder_path).glob("*.csv"):
            if "_merge" not in file.name:
                file.unlink()

        return merge_files_list
    else:
        return []


# è§£å‹å¹¶åˆ é™¤zipæ–‡ä»¶
def unzip_and_delete_zip(zip_files, folder_path):
    # æŸ¥æ‰¾æŒ‡å®šå¸ç§çš„zipæ–‡ä»¶
    for zip_file in zip_files:
        with zipfile.ZipFile(zip_file, 'r') as zip_ref:
            zip_ref.extractall(folder_path)  # è§£å‹
        # åˆ é™¤åŸzipæ–‡ä»¶
        # os.remove(zip_file)
    # print(f"{coin_name}è§£å‹å®Œæˆ")


def process_single_file(file):
    # è¯»å–æ–‡ä»¶çš„ç¬¬ä¸€è¡Œ
    with open(file, 'r') as f:
        first_line = f.readline().strip()  # .strip() ç§»é™¤å¯èƒ½çš„å‰åç©ºç™½å­—ç¬¦

    # æ£€æŸ¥ç¬¬ä¸€è¡Œæ˜¯å¦åŒ…å«ä»»ä½•é¢„æœŸçš„åˆ—å
    has_header = any(col_name in first_line for col_name in ["open_time", "open", "high", "low", "close"])

    # æ ¹æ®æ–‡ä»¶æ˜¯å¦æœ‰åˆ—åæ¥è¯»å–æ•°æ®
    df = pd.read_csv(file, header=0 if has_header else None)

    # å¸å®‰APIè¿”å›çš„éƒ¨åˆ†æ–‡ä»¶æ²¡æœ‰åˆ—åï¼Œå¦‚æœæ²¡æœ‰åˆ—åï¼Œéœ€è¦æ‰‹åŠ¨æŒ‡å®š
    if not has_header:
        df.columns = [
            "open_time", "open", "high", "low", "close", "volume",
            "close_time", "quote_volume", "count",
            "taker_buy_volume", "taker_buy_quote_volume", "ignore"
        ]
    # å°†åˆ—åæ˜ å°„åˆ°æ–°çš„åˆ—å
    column_mapping = {
        "open_time": "candle_begin_time",
        "count": "trade_num",
        "taker_buy_volume": "taker_buy_base_asset_volume",
        "taker_buy_quote_volume": "taker_buy_quote_asset_volume"
    }
    df.rename(columns=column_mapping, inplace=True)

    # æ·»åŠ æ–°çš„åˆ—
    df['symbol'] = file.split(os.sep)[-1].split('USDT')[0] + '-USDT'
    # æ³¨æ„ï¼šavg_price_1m å’Œ avg_price_5m éœ€è¦åç»­è®¡ç®—

    # è½¬æ¢æ—¶é—´æ ¼å¼
    df['candle_begin_time'] = pd.to_datetime(df['candle_begin_time'], unit='ms')

    # åˆ é™¤ä¸éœ€è¦çš„åˆ—
    df.drop(['close_time', 'ignore'], axis=1, inplace=True)

    return df


def process_coin_files(files):
    dataframes = Parallel(n_jobs=max(os.cpu_count() - 1, 1))(
        delayed(process_single_file)(file) for file in files
    )
    merged_df = pd.concat(dataframes)

    merged_df.sort_values(by='candle_begin_time', inplace=True)
    merged_df.drop_duplicates(subset=['candle_begin_time'], inplace=True, keep='last')
    merged_df.reset_index(drop=True, inplace=True)
    # å¡«å……ç©ºç¼ºçš„æ•°æ®
    start_date = merged_df.iloc[0]['candle_begin_time']
    end_date = merged_df.iloc[-1]['candle_begin_time']
    benchmark = pd.DataFrame(pd.date_range(start=start_date, end=end_date, freq='1T'))  # åˆ›å»ºå¼€å§‹è‡³å›æµ‹ç»“æŸæ—¶é—´çš„1Håˆ—è¡¨
    benchmark.rename(columns={0: 'candle_begin_time'}, inplace=True)
    merged_df = pd.merge(left=benchmark, right=merged_df, on='candle_begin_time', how='left', sort=True, indicator=True)
    merged_df['close'] = merged_df['close'].fillna(method='ffill')
    merged_df['symbol'] = merged_df['symbol'].fillna(method='ffill')
    for column in ['open', 'high', 'low']:
        merged_df[column] = merged_df[column].fillna(merged_df['close'])
    _ = ['volume', 'quote_volume', 'trade_num', 'taker_buy_base_asset_volume', 'taker_buy_quote_asset_volume']
    merged_df[_] = merged_df[_].fillna(0)
    # merged_df = merged_df[merged_df['_merge'] == 'left_only']

    # å°†ç´¢å¼•è½¬æ¢ä¸ºDatetimeIndexï¼Œå¦‚æœå®ƒè¿˜ä¸æ˜¯
    merged_df.set_index('candle_begin_time', inplace=True)

    merged_df['avg_price_1m'] = merged_df['quote_volume'] / merged_df['volume']
    merged_df['avg_price_5m'] = merged_df['quote_volume'].rolling(window=5).sum() / merged_df['volume'].rolling(
        window=5).sum()
    merged_df['avg_price_5m'] = merged_df['avg_price_5m'].shift(-4)
    merged_df['avg_price_1m'].fillna(merged_df['open'], inplace=True)
    merged_df['avg_price_5m'].fillna(merged_df['open'], inplace=True)

    hourly_df = merged_df.resample('1H').agg({
        'open': 'first',
        'high': 'max',
        'low': 'min',
        'close': 'last',
        'volume': 'sum',
        'quote_volume': 'sum',
        'trade_num': 'sum',
        'taker_buy_base_asset_volume': 'sum',
        'taker_buy_quote_asset_volume': 'sum',
        'symbol': 'first',
        'avg_price_1m': 'first',
        'avg_price_5m': 'first',
    })
    hourly_df.reset_index(inplace=True)

    return hourly_df


def find_trigger_index(goals, prices_list, trigger_type):
    """
    :param goals: ç›®æ ‡ä»·æ ¼åˆ—è¡¨
    :param prices_list: ä»·æ ¼åˆ—è¡¨
    :param trigger_type: è§¦å‘ç±»å‹ï¼Œ'loss'è¡¨ç¤ºæ­¢æŸï¼Œ'profit'è¡¨ç¤ºæ­¢ç›ˆ
    :return: è§¦å‘ç´¢å¼•åˆ—è¡¨
    """
    trigger_indexes = []
    for goal, prices in zip(goals, prices_list):
        if trigger_type == 'loss':
            trigger_index = next((i for i, price in enumerate(prices) if goal > price), None)
        elif trigger_type == 'profit':
            trigger_index = next((i for i, price in enumerate(prices) if goal < price), None)
        else:
            raise ValueError('æ­¢ç›ˆæ­¢æŸç±»å‹éæ³•')
        trigger_indexes.append(trigger_index if trigger_index is not None else float('inf'))
    return trigger_indexes


def calculate_stop(row):
    """
    è®¡ç®—æ­¢ç›ˆæ­¢æŸè§¦å‘æƒ…å†µ
    """
    if row['stop_loss_trigger'] == float('inf') and row['stop_profit_trigger'] == float('inf'):
        return 0  # æ­¢ç›ˆæ­¢æŸéƒ½æ²¡è§¦å‘
    elif row['stop_loss_trigger'] < row['stop_profit_trigger']:
        return -1  # æ­¢æŸå…ˆè§¦å‘
    elif row['stop_loss_trigger'] > row['stop_profit_trigger']:
        return 1  # æ­¢ç›ˆå…ˆè§¦å‘
    else:
        return 2  # åœ¨åŒä¸€å°æ—¶åŒæ—¶è§¦å‘æ­¢ç›ˆå’Œæ­¢æŸ


def process_single_combination(df, stop_profit, stop_loss):
    _ = df.copy()
    # print(f'stop_profit: {stop_profit}, stop_loss: {stop_loss}')
    _['stop_profit_price'] = _['avg_price_1m'] * (1 + stop_profit)
    _['stop_loss_price'] = _['avg_price_1m'] * (1 + stop_loss)
    # åˆ›å»ºæœ€é«˜ä»·å’Œæœ€ä½ä»·çš„è¾…åŠ©åˆ—ï¼Œå­˜å‚¨æœªæ¥24å°æ—¶çš„ä»·æ ¼æ•°ç»„
    low_list = []
    high_list = []
    for index, row in _.iterrows():
        high_prices = _.loc[index:index + 23, 'high'].tolist()
        low_prices = _.loc[index:index + 23, 'low'].tolist()
        extra_prices = _.loc[index + 24:index + 24, 'avg_price_1m'].tolist()
        high_list.append(high_prices + extra_prices)
        low_list.append(low_prices + extra_prices)
    _['high_list'] = high_list
    _['low_list'] = low_list
    _['stop_profit_trigger'] = find_trigger_index(_['stop_profit_price'], _['high_list'], 'profit')
    _['stop_loss_trigger'] = find_trigger_index(_['stop_loss_price'], _['low_list'], 'loss')

    # è®¡ç®—æœ€ç»ˆçš„stopå€¼
    _[f'stop[{stop_profit}_{stop_loss}]'] = _.apply(calculate_stop, axis=1)

    return _[f'stop[{stop_profit}_{stop_loss}]']


def process_stop(df, stop_loss_list, stop_profit_list, n_jobs=-1):
    if not calc_stop:
        return df
    # print(f'å¼€å§‹è®¡ç®—{coin_name}æ­¢ç›ˆæ­¢æŸçŠ¶æ€æ•°æ®')
    stop_all_list = list(product(stop_profit_list, stop_loss_list))
    results_dfs = Parallel(n_jobs=n_jobs)(
        delayed(process_single_combination)(df, stop_profit, stop_loss) for stop_profit, stop_loss in stop_all_list)
    results_combined = pd.concat(results_dfs, axis=1)
    # åˆå¹¶ç»“æœ
    df_final = pd.concat([df, results_combined], axis=1)

    return df_final


def get_merge_csv_files(folder_path):
    csv_files = glob(os.path.join(folder_path, '*.csv'))

    grouped_files = {}
    for file in csv_files:

        # if '_merge' in file:
        if '_merged' in os.path.basename(file):
            continue
        coin_name = os.path.basename(file).split('-')[0]

        grouped_files.setdefault(coin_name, []).append(file)

    for coin_name, files in grouped_files.items():
        try:
            hourly_df = process_coin_files(files)

            df_final = process_stop(hourly_df, stop_loss_list, stop_profit_list)

            df_final.to_csv(os.path.join(folder_path, f'{coin_name}_merged.csv'), index=False)

        except Exception as exc:
            print(f"\n {coin_name}ç”Ÿæˆè¿‡ç¨‹ä¸­å‡ºé”™: {exc}")


# åˆ é™¤æœªåˆå¹¶çš„CSVæ–‡ä»¶
def delete_unmerged_csv_files(folder_path):
    # æŸ¥æ‰¾æ–‡ä»¶å¤¹ä¸­æ‰€æœ‰çš„CSVæ–‡ä»¶
    csv_files = glob(os.path.join(folder_path, '*.csv'))

    for csv_file in csv_files:
        # æ£€æŸ¥æ–‡ä»¶åæ˜¯å¦ä¸åŒ…å« '_merged'
        if '_merged' not in os.path.basename(csv_file):
            os.remove(csv_file)  # åˆ é™¤æ–‡ä»¶


if __name__ == "__main__":
    target = 'spot'
    # æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿçš„å‘½ä»¤è¡Œå‚æ•°
    if len(sys.argv) > 1:
        target = sys.argv[1]

    coins_to_clean = extract_coin_names(ä¸‹è½½æ–‡ä»¶å¤¹)
    merge_csvs = all_merge_csv(ä¸‹è½½æ–‡ä»¶å¤¹)

    # å¦‚æœä»»åŠ¡ä¸­æ–­ï¼Œè¯†åˆ«æ–­ç‚¹ï¼Œç»§ç»­æ¸…ç†
    if len(merge_csvs) >= 2:
        index_next_clean = coins_to_clean.index(merge_csvs[-2])
        coins_to_clean = coins_to_clean[index_next_clean:]

    mode = "ç°è´§æ•°æ®" if target == "spot" else "åˆçº¦æ•°æ®"
    with tqdm(total=len(coins_to_clean), desc="æ€»ä½“è¿›åº¦", unit="step") as pbar:
        for coin_name in coins_to_clean:
            # æ­¥éª¤1: è§£å‹
            zip_files = glob(os.path.join(ä¸‹è½½æ–‡ä»¶å¤¹, f'{coin_name}*.zip'))
            file_num = len(zip_files)
            pbar.set_description(f"ğŸ“¦ æ­£åœ¨è§£å‹{file_num}ä¸ª{coin_name}çš„zipæ–‡ä»¶")
            unzip_and_delete_zip(zip_files, ä¸‹è½½æ–‡ä»¶å¤¹)  # è§£å‹æŒ‡å®šå¸ç§çš„zipæ–‡ä»¶å¹¶åˆ é™¤

            # æ­¥éª¤2: æ¸…æ´—åˆå¹¶
            pbar.set_description(f"ğŸ”„ æ­£åœ¨æ¸…æ´—åˆå¹¶{coin_name}çš„{file_num}ä¸ªKçº¿æ•°æ®csvæ–‡ä»¶")
            get_merge_csv_files(ä¸‹è½½æ–‡ä»¶å¤¹)

            # æ­¥éª¤3: åˆ é™¤è¿™ä¸ªå¸ç§çš„ä¸€åˆ†é’ŸCSV,å®Œæˆå¤„ç†
            delete_unmerged_csv_files(ä¸‹è½½æ–‡ä»¶å¤¹)
            pbar.update(1)
            pbar.set_description(f"ğŸ’¯ {file_num}ä¸ª{coin_name}{mode}ï¸æ¸…æ´—å®Œæˆï¼Œå·²åˆå¹¶ä¿å­˜")
            print('')
            time.sleep(1)
    pbar.close()
    print("\næ‰€æœ‰å¸ç§æ¸…æ´—å®Œæˆ")

